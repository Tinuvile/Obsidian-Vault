
问题可能出在 librdkafka 的构建配置上。让我们尝试以下解决方案：

1. 首先清理之前的构建缓存：

```bash
cargo clean
``` 

2. 然后安装一些必要的开发依赖：

```bash
sudo apt update
sudo apt install -y build-essential pkg-config libzstd-dev libssl-dev libsasl2-dev liblz4-dev zlib1g-dev
```

3. 设置一些环境变量来强制使用特定的 CMake 策略版本：

```bash
export CMAKE_POLICY_VERSION_MINIMUM=3.5
export OPENSSL_ROOT_DIR=/usr
```

4. 重新尝试构建项目
   
   依次运行
```bash
cargo run --package cmd --bin placement-center -- --conf=config/placement-center.toml
cargo run --package cmd --bin mqtt-server -- --conf=config/mqtt-server.toml
cargo run --package cmd --bin journal-server -- --conf=config/journal-server.toml
```
   
   - **placement-center**
```bash
tinuvile@LAPTOP-7PVP3HH3:~/RobustMQ$ cargo run --package cmd --bin placement-center -- --conf=config/placement-center.toml
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.44s
     Running `target/debug/placement-center --conf=config/placement-center.toml`
2025-05-14T06:45:57.412264Z  INFO openraft::storage::helper: get_initial_state vote=<T1-N1:Q> last_purged_log_id=None last_applied=None committed=T1-N1.1 last_log_id=T1-N1.1
2025-05-14T06:45:57.412515Z  INFO openraft::storage::helper: re-apply log [0..2) in 64 item chunks to state machine
2025-05-14T06:45:57.412911Z  INFO openraft::storage::helper: re-apply 2 log entries: [0, 2),
2025-05-14T06:45:57.413149Z  INFO openraft::storage::helper: load membership from log: [2..2)
2025-05-14T06:45:57.413353Z  INFO openraft::storage::helper: load key log ids from (None,T1-N1.1]
2025-05-14T06:45:57.413943Z  INFO openraft::engine::engine_impl: startup begin: state: RaftState { vote: Leased { data: Vote { leader_id: LeaderId { term: 1, node_id: 1 }, committed: true }, last_update: Some(Instant { tv_sec: 4473, tv_nsec: 350950781 }), lease: 0ns, lease_enabled: true }, purged_next: 0, log_ids: LogIdList { key_log_ids: [LogId { leader_id: LeaderId { term: 0, node_id: 0 }, index: 0 }, LogId { leader_id: LeaderId { term: 1, node_id: 1 }, index: 1 }] }, membership_state: MembershipState { committed: EffectiveMembership { log_id: Some(LogId { leader_id: LeaderId { term: 0, node_id: 0 }, index: 0 }), membership: Membership { configs: [{1}], nodes: {1: Node { node_id: 1, rpc_addr: "127.0.0.1:1228" }} }, voter_ids: {1} }, effective: EffectiveMembership { log_id: Some(LogId { leader_id: LeaderId { term: 0, node_id: 0 }, index: 0 }), membership: Membership { configs: [{1}], nodes: {1: Node { node_id: 1, rpc_addr: "127.0.0.1:1228" }} }, voter_ids: {1} } }, snapshot_meta: SnapshotMeta { last_log_id: None, last_membership: StoredMembership { log_id: None, membership: Membership { configs: [], nodes: {} } }, snapshot_id: "" }, server_state: Learner, io_state: IOState { building_snapshot: false, log_progress: IOProgress { accepted: Some(Log(LogIOId { committed_vote: CommittedVote { leader_id: LeaderId { term: 1, node_id: 1 } }, log_id: None })), submitted: Some(Log(LogIOId { committed_vote: CommittedVote { leader_id: LeaderId { term: 1, node_id: 1 } }, log_id: None })), flushed: Some(Log(LogIOId { committed_vote: CommittedVote { leader_id: LeaderId { term: 1, node_id: 1 } }, log_id: None })), name: "LogIO" }, apply_progress: IOProgress { accepted: Some(LogId { leader_id: LeaderId { term: 1, node_id: 1 }, index: 1 }), submitted: Some(LogId { leader_id: LeaderId { term: 1, node_id: 1 }, index: 1 }), flushed: Some(LogId { leader_id: LeaderId { term: 1, node_id: 1 }, index: 1 }), name: "Apply" }, snapshot: None, purged: None }, purge_upto: None }, is_leader: true, is_voter: true
2025-05-14T06:45:57.414086Z  INFO placement_center: Initiate Monitoring of Raft Leader Transitions
2025-05-14T06:45:57.414350Z  INFO openraft::engine::handler::server_state_handler: become leader id=1
2025-05-14T06:45:57.414472Z  INFO placement_center::server::grpc::server: RobustMQ Meta Grpc Server start success. bind addr:127.0.0.1:1228
2025-05-14T06:45:57.414484Z  INFO openraft::core::raft_core: remove all replication
2025-05-14T06:45:57.414674Z  INFO openraft::core::heartbeat::handle: id=1 HeartbeatWorker are shutdown
2025-05-14T06:45:57.414806Z  INFO placement_center::raft::raft_node: Raft Nodes:{1: Node { node_id: 1, rpc_addr: "127.0.0.1:1228" }}
2025-05-14T06:45:57.415252Z  INFO placement_center::raft::leadership: Leader transition has occurred. current leader is Node Some(1). Previous leader was Node None.
2025-05-14T06:45:57.415308Z  INFO placement_center::raft::raft_node: Whether nodes should be initialized, flag=true
2025-05-14T06:45:57.415716Z  INFO placement_center::journal::controller: Storage Engine Controller started successfully
2025-05-14T06:45:57.416644Z  INFO common_base::metrics: Prometheus HTTP Server started successfully, listening port: 9090
2025-05-14T06:45:57.421016Z  INFO placement_center: Placement Center service started successfully...
2025-05-14T06:46:21.260101Z  INFO placement_center::server::grpc::service_inner: register node:RegisterNodeRequest { cluster_type: MqttBrokerServer, cluster_name: "mqtt-broker", node_ip: "100.80.72.94", node_id: 1, node_inner_addr: "100.80.72.94:9981", extend_info: "{\"grpc_addr\":\"100.80.72.94:9981\",\"mqtt_addr\":\"100.80.72.94:1883\",\"mqtts_addr\":\"100.80.72.94:8883\",\"websocket_addr\":\"100.80.72.94:8093\",\"websockets_addr\":\"100.80.72.94:8094\",\"quic_addr\":\"100.80.72.94:9083\"}" }
2025-05-14T06:46:21.282780Z  INFO placement_center::mqtt::controller::call_broker: Thread starts successfully, Inner communication between Placement Center and MQTT Broker node [BrokerNode { cluster_name: "mqtt-broker", cluster_type: "MQTTBrokerServer", create_time: 1747205181260, extend: "{\"grpc_addr\":\"100.80.72.94:9981\",\"mqtt_addr\":\"100.80.72.94:1883\",\"mqtts_addr\":\"100.80.72.94:8883\",\"websocket_addr\":\"100.80.72.94:8093\",\"websockets_addr\":\"100.80.72.94:8094\",\"quic_addr\":\"100.80.72.94:9083\"}", node_id: 1, node_inner_addr: "100.80.72.94:9981", node_ip: "100.80.72.94" }].
2025-05-14T06:46:59.867415Z  INFO placement_center::server::grpc::service_inner: register node:RegisterNodeRequest { cluster_type: JournalServer, cluster_name: "JournalCluster1", node_ip: "127.0.0.1", node_id: 1, node_inner_addr: "127.0.0.1:2228", extend_info: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}" }
2025-05-14T06:46:59.902201Z  INFO placement_center::journal::controller::call_node: Thread starts successfully, Inner communication between Placement Center and Journal Engine node [BrokerNode { cluster_name: "JournalCluster1", cluster_type: "JournalServer", create_time: 1747205219867, extend: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}", node_id: 1, node_inner_addr: "127.0.0.1:2228", node_ip: "127.0.0.1" }].
2025-05-14T06:47:06.530833Z  INFO placement_center::core::heartbeat: Heartbeat of the Node times out and is deleted from the cluster. Node ID: 1, node IP: 127.0.0.1,now time:1747205226,report time:1747205221
2025-05-14T06:47:06.531029Z  INFO placement_center::journal::controller::call_node: Thread stops successfully, Inner communication between Placement Center and Journal Engine node [BrokerNode { cluster_name: "JournalCluster1", cluster_type: "JournalServer", create_time: 1747205219867, extend: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}", node_id: 1, node_inner_addr: "127.0.0.1:2228", node_ip: "127.0.0.1" }].
2025-05-14T06:47:11.915534Z  INFO placement_center::server::grpc::service_inner: register node:RegisterNodeRequest { cluster_type: JournalServer, cluster_name: "JournalCluster1", node_ip: "127.0.0.1", node_id: 1, node_inner_addr: "127.0.0.1:2228", extend_info: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}" }
2025-05-14T06:47:11.938555Z  INFO placement_center::journal::controller::call_node: Thread starts successfully, Inner communication between Placement Center and Journal Engine node [BrokerNode { cluster_name: "JournalCluster1", cluster_type: "JournalServer", create_time: 1747205231915, extend: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}", node_id: 1, node_inner_addr: "127.0.0.1:2228", node_ip: "127.0.0.1" }].

```

  - **mqtt-server**
  
```bash
tinuvile@LAPTOP-7PVP3HH3:~/RobustMQ$ cargo run --package cmd --bin mqtt-server -- --conf=config/mqtt-server.toml
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 0.42s
     Running `target/debug/mqtt-server --conf=config/mqtt-server.toml`
2025-05-14T06:46:21.269341Z  INFO mqtt_broker: Node 1 has been successfully registered
2025-05-14T06:46:21.269860Z  INFO mqtt_broker::handler::sub_parse_topic: Subscribe manager thread started successfully.
2025-05-14T06:46:21.270150Z  INFO mqtt_broker::server::grpc::server: Broker Grpc Server start success. port:9981
2025-05-14T06:46:21.270481Z  INFO mqtt_broker::server::websocket::server: Broker WebSocket Server start success. port:8093
2025-05-14T06:46:21.270827Z  INFO delay_message::delay: init shard:mqtt-broker, $delay-message-shard-0
2025-05-14T06:46:21.271474Z  INFO delay_message::persist: Delay queue index was successfully constructed from the persistent store. Number of data items: 0
2025-05-14T06:46:21.272874Z  INFO mqtt_broker::server::websocket::server: Broker WebSocket TLS Server start success. port:8094
2025-05-14T06:46:21.273013Z  INFO mqtt_broker::server::tcp::server: MQTT TCP Server started successfully, listening port: 1883
2025-05-14T06:46:21.277391Z  INFO mqtt_broker: MQTT Broker service started successfully...
2025-05-14T06:46:21.296359Z  INFO mqtt_broker::server::tcp::server: MQTT TCP TLS Server started successfully, listening port: 8883
2025-05-14T06:46:21.329388Z  INFO mqtt_broker::server::quic::server: MQTT Quic Server started successfully, listening port: 9083
2025-05-14T06:46:21.338929Z  INFO common_base::metrics: Prometheus HTTP Server started successfully, listening port: 9091
2025-05-14T06:46:23.290783Z  INFO mqtt_broker::server::grpc::inner: update cache, resource_type:Topic,action_type:Set
2025-05-14T06:46:23.322418Z  INFO mqtt_broker::server::grpc::inner: update cache, resource_type:Topic,action_type:Set
2025-05-14T06:46:23.337362Z  INFO mqtt_broker::server::grpc::inner: update cache, resource_type:Topic,action_type:Set
2025-05-14T06:46:23.348404Z  INFO mqtt_broker::server::grpc::inner: update cache, resource_type:Topic,action_type:Set
2025-05-14T06:46:23.368160Z  INFO mqtt_broker::server::grpc::inner: update cache, resource_type:Topic,action_type:Set  
```

  - **journal-server**

```bash
tinuvile@LAPTOP-7PVP3HH3:~/RobustMQ$ cargo run --package cmd --bin journal-server -- --conf=config/journal-server.toml
   Compiling cmd v0.1.14 (/home/tinuvile/RobustMQ/src/cmd)
    Finished `dev` profile [unoptimized + debuginfo] target(s) in 21.74s
     Running `target/debug/journal-server --conf=config/journal-server.toml`
2025-05-14T06:46:58.845209Z  INFO journal_server::core::cache: cluster config: {"enable_auto_create_shard":false,"shard_replica_num":1,"max_segment_size":1048576,"last_update_local_cache_time":0}
2025-05-14T06:46:58.845257Z  INFO journal_server::server::grpc::server: Journal Engine Grpc Server start success. addr:127.0.0.1:2228
2025-05-14T06:46:58.847631Z  INFO journal_server::server::tcp::server: Journal Engine Server started successfully, addr: 127.0.0.1:3110
2025-05-14T06:46:58.851350Z  INFO journal_server::core::cache: Load the node cache, the number of nodes is 0
2025-05-14T06:46:58.856318Z  INFO journal_server::core::cache: Load the shard cache, the number of shards is 0
2025-05-14T06:46:58.859706Z  INFO journal_server::core::cache: Load the segment cache, the number of segments is 0
2025-05-14T06:46:58.862655Z  INFO journal_server::core::cache: Load the segment metadata cache, the number of segments is 0
2025-05-14T06:46:58.872174Z  INFO journal_server::server::tcp::server: Journal Engine TLS Server started successfully, addr: 127.0.0.1:3111
2025-05-14T06:47:01.906192Z  INFO journal_server: Journal Node was initialized successfully
2025-05-14T06:47:01.906749Z  INFO journal_server::segment::scroll: Segment scroll thread started successfully
2025-05-14T06:47:01.909257Z  INFO journal_server::core::notification: Update the cache, Set node, node: BrokerNode { cluster_name: "JournalCluster1", cluster_type: "JournalServer", create_time: 1747205219867, extend: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}", node_id: 1, node_inner_addr: "127.0.0.1:2228", node_ip: "127.0.0.1" }
2025-05-14T06:47:13.942782Z  INFO journal_server::core::notification: Update the cache, Set node, node: BrokerNode { cluster_name: "JournalCluster1", cluster_type: "JournalServer", create_time: 1747205231915, extend: "{\"data_fold\":[\"./robust-data/journal-server/storage/data1\",\"./robust-data/journal-server/storage/data2\"],\"tcp_addr\":\"127.0.0.1:3110\",\"tcps_addr\":\"127.0.0.1:3111\"}", node_id: 1, node_inner_addr: "127.0.0.1:2228", node_ip: "127.0.0.1" }
```


根据日志信息，**所有服务均已成功启动**，以下是各服务的运行状态验证：

---

### **服务状态验证**

#### **1. Placement Center (元数据服务)**
- **Raft 共识引擎**  
  ✓ 成功选举为 Leader（`become leader id=1`）  
  ✓ 监听 gRPC 端口 `127.0.0.1:1228`  
  ✓ Prometheus 监控端口 `9090` 已启用  
- **节点管理**  
  ✓ 接受 MQTT Broker 节点注册（`node_id=1`, `100.80.72.94:9981`）  
  ✓ 接受 Journal Server 节点注册（`node_id=1`, `127.0.0.1:2228`）  
  ✓ 检测到心跳超时后自动恢复（`06:47:06` 超时，`06:47:11` 重新注册）  

#### **2. MQTT Broker (消息代理服务)**
- **网络服务**  
  ✓ TCP 服务端口 `1883` (明文)、`8883` (TLS)  
  ✓ WebSocket 服务端口 `8093` (明文)、`8094` (TLS)  
  ✓ QUIC 服务端口 `9083`  
  ✓ gRPC 内部通信端口 `9981`  
  ✓ Prometheus 监控端口 `9091`  
- **功能验证**  
  ✓ 订阅管理器线程启动成功  
  ✓ 延迟消息持久化初始化完成（`delay queue index`）  

#### **3. Journal Server (存储引擎服务)**
- **存储组件**  
  ✓ gRPC 服务端口 `2228`  
  ✓ TCP 存储服务端口 `3110`、TLS 端口 `3111`  
  ✓ 分片滚动线程启动  
- **数据管理**  
  ✓ 初始化节点/分片/段元数据缓存  
  ✓ 接收 Placement Center 的节点配置更新  

---

### **关键成功指标**

| **指标**               | **Placement Center** | **MQTT Broker** | **Journal Server** |  
|------------------------|----------------------|-----------------|--------------------|
| 核心服务端口监听        | ✓ (1228)            | ✓ (1883/8883)  | ✓ (2228/3110)     |  
| 集群节点注册功能        | ✓                   | ✓               | ✓                 |  
| 内部通信线程启动        | ✓                   | ✓               | ✓                 |  
| 监控指标暴露            | ✓ (9090)            | ✓ (9091)        | -                 |  
| 错误日志                | 1 次心跳超时*        | 无              | 无                |  

*注：心跳超时为节点 `127.0.0.1:2228` 在 `06:47:06` 的临时故障，后续已自动恢复。

---

### **后续操作建议**

1. **功能验证**  
   ```bash
   # 测试 MQTT 客户端连接
   mosquitto_pub -h 100.80.72.94 -p 1883 -t test -m "hello"

   # 查询 Placement Center 节点列表
   grpcurl -plaintext 127.0.0.1:1228 list
   ```

2. **监控检查**  
   - 访问 `http://localhost:9090/metrics` 查看元数据服务指标  
   - 访问 `http://localhost:9091/metrics` 查看 Broker 指标  

3. **日志跟踪**  
   ```bash
   # 实时查看 Placement Center 日志
   tail -f ~/RobustMQ/logs/placement_center.log

   # 过滤 WARN/ERROR 级别日志
   grep -E 'WARN|ERROR' ~/RobustMQ/logs/*.log
   ```

4. **配置优化**  
   - 调整 `config/placement-center.toml` 中的 `heartbeat_interval` 防止误判  
   - 确保所有服务的 `node_ip` 配置为实际网络 IP（非 `127.0.0.1`）

---

### **故障排查指引**

| **现象**               | **可能原因**          | **解决措施**                      |  
|------------------------|----------------------|----------------------------------|  
| 节点频繁心跳超时        | 网络延迟过高          | 调整 `heartbeat_timeout` 参数     |  
| gRPC 连接失败          | 端口冲突/防火墙拦截   | 检查 `netstat -tulnp \| grep <端口>` |  
| 存储引擎未持久化数据    | 数据目录权限不足      | `chmod -R 755 ./robust-data`     |  

---

当前所有服务日志均显示 **正常运行状态**，可以开始进行业务测试。建议重点关注节点间网络稳定性及存储目录权限配置。